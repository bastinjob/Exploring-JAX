# Exploring JAX for ML Model Optimization

This project aims to compare the performance of ML models built using JAX against models implemented in traditional libraries (NumPy, TensorFlow, PyTorch). We will study how JAX improves computation speed, memory usage, and scalability, particularly in GPU and TPU environments.

## Structure
- `src/`: Contains source code for models and utilities.
- `results/`: Stores timing results, memory benchmarks, and plots.
- `notebooks/`: Jupyter notebooks for experimental analysis.
- `data/`: Placeholder for any datasets.

## Objectives
- Compare training times between JAX and other libraries.
- Analyze memory efficiency of JAX for large models.
- Evaluate inference speed and suitability for real-time systems.

## Usage
1. Install dependencies using `requirements.txt`.
2. Run the models using the scripts in the `src/` folder.